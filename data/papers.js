papers = {"data":[{"citation":"Moody, Christopher E. <strong>\"Mixing dirichlet topic models and word embeddings to make lda2vec.\"<\/strong> arXiv preprint arXiv:1605.02019 (2016).","link":"https:\/\/arxiv.org\/abs\/1605.02019","notes":"Dug really deep into this paper as I was learning more and more about generating my own embedding spaces. Implemented this paper and found it very difficult to get quality results. Seems like this is a common problem and the original implementation does seem to work either (supported by issues and another implementor's results). Maybe there is a training problem when jointly learning distributions or compositions of distributions?","last_updated":1575046256.9375832081,"created_at":1575045754.5449368954,"id":"5ffed44a-ebe2-4172-a76e-40cd1281ba02"},{"citation":"Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. <strong>\"Mine: mutual information neural estimation.\"<\/strong> arXiv preprint arXiv:1801.04062, ICML\u20192018, 2018.","link":"https:\/\/arxiv.org\/abs\/1801.04062","notes":"Very cool approach. Makes me want to dig more into information theory-based ML papers. Gotta see what this whole Information Bottleneck thing is.","last_updated":1575046283.3221960068,"created_at":1575045958.8890528679,"id":"44a2dfb3-2e1c-4f99-8e97-d66e35686c1a"},{"citation":"Hjelm, R. Devon, et al. <strong>\"Learning deep representations by mutual information estimation and maximization.\"<\/strong> arXiv preprint arXiv:1808.06670 (2018).","link":"https:\/\/arxiv.org\/abs\/1808.06670","notes":"Natural extension to MINE with the same clarity.","created_at":1575058375.1809160709,"id":"5df9bee1-e40c-464a-b98c-896de8a5f766"},{"citation":"Frankle, Jonathan, and Michael Carbin. <strong>\"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\"<\/strong> arXiv preprint arXiv:1803.03635 (2018).","link":"https:\/\/arxiv.org\/abs\/1803.03635","notes":"First paper I ever lead a paper discussion on in spring of my freshman year - found the paper on arxiv (lucky catch)! One of those papers with a clear simple idea that demonstrably works.","created_at":1575135390.5018129349,"id":"c0b9a47d-ad29-4f0a-aa1b-63e20f10d84a"},{"citation":"Schwartz, Martin A. <strong>\"The importance of stupidity in scientific research.\"<\/strong> Journal of Cell Science 121.11 (2008): 1771-1771.","link":"https:\/\/jcs.biologists.org\/content\/joces\/121\/11\/1771.full.pdf","notes":"Gives focus to what is important.","created_at":1575328625.8627240658,"id":"f78ad082-1712-4fb0-a47e-fc3f708ced7d"},{"citation":"Petsiuk, Vitali, Abir Das, and Kate Saenko. <strong>\"Rise: Randomized input sampling for explanation of black-box models.\"<\/strong> arXiv preprint arXiv:1806.07421 (2018).","link":"https:\/\/arxiv.org\/abs\/1806.07421","notes":"A logically sound paper on generating saliency maps in a model-agnostic manner. Dug pretty deep into this one about a year ago.","created_at":1575392368.1973569393,"id":"7be33e65-3763-457b-ab8b-95b72c25800a"},{"citation":"He, Di, et al. <strong>\"Dual learning for machine translation.\"<\/strong> Advances in Neural Information Processing Systems. 2016.","link":"https:\/\/arxiv.org\/abs\/1611.00179","notes":"Introduced me to the ideal of dual learning - interesting approach to capturing information.","created_at":1575611124.6176400185,"id":"d0d50925-eedc-46a9-8418-60c90193844b"},{"citation":"Bau, David, et al. <strong>\"Gan dissection: Visualizing and understanding generative adversarial networks.\"<\/strong> arXiv preprint arXiv:1811.10597 (2018).","link":"https:\/\/arxiv.org\/abs\/1811.10597","notes":"\"How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\"","created_at":1575684929.1872570515,"id":"5f254568-2334-40c7-a77d-bed7a172fa33"},{"citation":"Zhou, Hattie, et al. <strong>\"Deconstructing lottery tickets: Zeros, signs, and the supermask.\"<\/strong> arXiv preprint arXiv:1905.01067 (2019).","link":"https:\/\/arxiv.org\/abs\/1905.01067","notes":"First paper to find that random subnetworks that do well on a task do exist (that I've seen) - i.e. supermasks","created_at":1576948168.206610918,"id":"b47d137d-6974-4e2d-9b8b-13f9e0824b10"},{"citation":"Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., & Rastegari, M. <strong>\"What\u2019s Hidden in a Randomly Weighted Neural Network?\"<\/strong> arXiv preprint arXiv:1911.13299 (2019)","link":"https:\/\/arxiv.org\/pdf\/1911.13299.pdf","notes":"Offers a \"complementary conjecture\" to the lottery ticket hypothesis and continues to show that a lot of useful stuff is already in a random network. \r\n\r\nCould probably replace the score gradient update with a Montecarlo approximation like the one used in RISE? - Tried this - did not work.\r\n\r\nReverse NEAT - start at some large network and prune down?","last_updated":1576966824.7382230759,"created_at":1575392192.8821098804,"id":"5069be6e-78a9-4b61-a4a6-6c016465cf14"}],"last_updated":1576966824.7387199402}