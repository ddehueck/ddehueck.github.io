papers = {"data":[{"citation":"Moody, Christopher E. <strong>\"Mixing dirichlet topic models and word embeddings to make lda2vec.\"<\/strong> arXiv preprint arXiv:1605.02019 (2016).","link":"https:\/\/arxiv.org\/abs\/1605.02019","notes":"Dug really deep into this paper as I was learning more and more about generating my own embedding spaces. Implemented this paper and found it very difficult to get quality results. Seems like this is a common problem and the original implementation does seem to work either (supported by issues and another implementor's results). Maybe there is a training problem when jointly learning distributions or compositions of distributions?","last_updated":1575046256.9375832081,"created_at":1575045754.5449368954,"id":"5ffed44a-ebe2-4172-a76e-40cd1281ba02"},{"citation":"Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. <strong>\"Mine: mutual information neural estimation.\"<\/strong> arXiv preprint arXiv:1801.04062, ICML\u20192018, 2018.","link":"https:\/\/arxiv.org\/abs\/1801.04062","notes":"Very cool approach. Makes me want to dig more into information theory-based ML papers. Gotta see what this whole Information Bottleneck thing is.","last_updated":1575046283.3221960068,"created_at":1575045958.8890528679,"id":"44a2dfb3-2e1c-4f99-8e97-d66e35686c1a"},{"citation":"Hjelm, R. Devon, et al. <strong>\"Learning deep representations by mutual information estimation and maximization.\"<\/strong> arXiv preprint arXiv:1808.06670 (2018).","link":"https:\/\/arxiv.org\/abs\/1808.06670","notes":"Natural extension to MINE with the same clarity.","created_at":1575058375.1809160709,"id":"5df9bee1-e40c-464a-b98c-896de8a5f766"},{"citation":"Frankle, Jonathan, and Michael Carbin. <strong>\"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\"<\/strong> arXiv preprint arXiv:1803.03635 (2018).","link":"https:\/\/arxiv.org\/abs\/1803.03635","notes":"First paper I ever lead a paper discussion on in spring of my freshman year - found the paper on arxiv (lucky catch)! One of those papers with a clear simple idea that demonstrably works.","created_at":1575135390.5018129349,"id":"c0b9a47d-ad29-4f0a-aa1b-63e20f10d84a"},{"citation":"Schwartz, Martin A. <strong>\"The importance of stupidity in scientific research.\"<\/strong> Journal of Cell Science 121.11 (2008): 1771-1771.","link":"https:\/\/jcs.biologists.org\/content\/joces\/121\/11\/1771.full.pdf","notes":"Gives focus to what is important.","created_at":1575328625.8627240658,"id":"f78ad082-1712-4fb0-a47e-fc3f708ced7d"},{"citation":"Petsiuk, Vitali, Abir Das, and Kate Saenko. <strong>\"Rise: Randomized input sampling for explanation of black-box models.\"<\/strong> arXiv preprint arXiv:1806.07421 (2018).","link":"https:\/\/arxiv.org\/abs\/1806.07421","notes":"A logically sound paper on generating saliency maps in a model-agnostic manner. Dug pretty deep into this one about a year ago.","created_at":1575392368.1973569393,"id":"7be33e65-3763-457b-ab8b-95b72c25800a"},{"citation":"He, Di, et al. <strong>\"Dual learning for machine translation.\"<\/strong> Advances in Neural Information Processing Systems. 2016.","link":"https:\/\/arxiv.org\/abs\/1611.00179","notes":"Introduced me to the ideal of dual learning - interesting approach to capturing information.","created_at":1575611124.6176400185,"id":"d0d50925-eedc-46a9-8418-60c90193844b"},{"citation":"Bau, David, et al. <strong>\"Gan dissection: Visualizing and understanding generative adversarial networks.\"<\/strong> arXiv preprint arXiv:1811.10597 (2018).","link":"https:\/\/arxiv.org\/abs\/1811.10597","notes":"\"How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\"","created_at":1575684929.1872570515,"id":"5f254568-2334-40c7-a77d-bed7a172fa33"},{"citation":"Zhou, Hattie, et al. <strong>\"Deconstructing lottery tickets: Zeros, signs, and the supermask.\"<\/strong> arXiv preprint arXiv:1905.01067 (2019).","link":"https:\/\/arxiv.org\/abs\/1905.01067","notes":"First paper to find that random subnetworks that do well on a task do exist (that I've seen) - i.e. supermasks","created_at":1576948168.206610918,"id":"b47d137d-6974-4e2d-9b8b-13f9e0824b10"},{"citation":"Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., & Rastegari, M. <strong>\"What\u2019s Hidden in a Randomly Weighted Neural Network?\"<\/strong> arXiv preprint arXiv:1911.13299 (2019)","link":"https:\/\/arxiv.org\/pdf\/1911.13299.pdf","notes":"Offers a \"complementary conjecture\" to the lottery ticket hypothesis and continues to show that a lot of useful stuff is already in a random network. \r\n\r\nCould probably replace the score gradient update with a Montecarlo approximation like the one used in RISE? - Tried this - did not work.\r\n\r\nReverse NEAT - start at some large network and prune down?","last_updated":1576966824.7382230759,"created_at":1575392192.8821098804,"id":"5069be6e-78a9-4b61-a4a6-6c016465cf14"},{"citation":"Schwarz, Jonathan, et al. <strong>\"Progress & compress: A scalable framework for continual learning.\"<\/strong> arXiv preprint arXiv:1805.06370 (2018).","link":"https:\/\/arxiv.org\/abs\/1805.06370","notes":"Having a knowledge base is an interesting idea, but the experimental results are not all that compelling. It seems most approaches against CF can't compete with single model accuracy except for progressive nets (which don't scale). Lays out clear desiderata though.","created_at":1577034986.3405959606,"id":"a685c77f-95af-40c2-8dba-b85932a210bc"},{"citation":"Leclerc, Guillaume, et al. <strong>\"Smallify: Learning network size while training.\"<\/strong> arXiv preprint arXiv:1806.03723 (2018).","link":"https:\/\/arxiv.org\/abs\/1806.03723","notes":"Shrinks network during training - optimizes a mask while optimizing performance as well. Includes a garbage collector for neurons to speed up inference. Can we find lottery in the same time it takes to training one network?","created_at":1577043964.6464288235,"id":"9d11d06b-386a-4787-9d48-10a13c9aeedc"},{"citation":"Ji, Xu, Jo\u00e3o F. Henriques, and Andrea Vedaldi. <strong>\"Invariant information clustering for unsupervised image classification and segmentation.\"<\/strong> Proceedings of the IEEE International Conference on Computer Vision. 2019.","link":"https:\/\/arxiv.org\/pdf\/1807.06653v4.pdf","notes":"Very strong results while poking holes in past MI approaches such as MINE. Will want to dig deeper - unclear to me how joint is estimated.","created_at":1577159804.2928729057,"id":"a319cd6e-29c1-41fa-978e-82c076567b18"},{"citation":"Brendel, Wieland, and Matthias Bethge. <strong>\"Approximating cnns with bag-of-local-features models works surprisingly well on imagenet.\"<\/strong> arXiv preprint arXiv:1904.00760 (2019).","link":"https:\/\/arxiv.org\/abs\/1904.00760","notes":"These similarities suggest that current network architectures base their decisions on a large number of relatively weak and local statistical regularities and are not sufficiently encouraged.\r\n\r\nWhat about bag-of-word models?","created_at":1577290932.3185889721,"id":"43d3d1e1-5a75-4500-9bda-1010c846439c"},{"citation":"Chollet, Fran\u00e7ois. <strong>\"The Measure of Intelligence.\"<\/strong> arXiv preprint arXiv:1911.01547 (2019).","link":"https:\/\/arxiv.org\/abs\/1911.01547","notes":"A very philosophical paper that attacks competition based science that has dominated DL since its revitalization in the 2000s. Focuses on providing an actionable measure of intelligence. Dataset put forward (ARC) has some issues but overall is a great step towards getting the ML community thinking more about intelligence and less about SOTA.","created_at":1577587788.2892839909,"id":"29f74a57-64c2-4ccc-a11c-3fec7b7e9a8e"}],"last_updated":1577587788.2942709923}