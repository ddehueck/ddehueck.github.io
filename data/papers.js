papers = {"data":[{"citation":"Moody, Christopher E. <strong>\"Mixing dirichlet topic models and word embeddings to make lda2vec.\"<\/strong> arXiv preprint arXiv:1605.02019 (2016).","link":"https:\/\/arxiv.org\/abs\/1605.02019","notes":"Dug really deep into this paper as I was learning more and more about generating my own embedding spaces. Implemented this paper and found it very difficult to get quality results. Seems like this is a common problem and the original implementation does seem to work either (supported by issues and another implementor's results). Maybe there is a training problem when jointly learning distributions or compositions of distributions?","last_updated":1575046256.9375832081,"created_at":1575045754.5449368954,"id":"5ffed44a-ebe2-4172-a76e-40cd1281ba02"},{"citation":"Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. <strong>\"Mine: mutual information neural estimation.\"<\/strong> arXiv preprint arXiv:1801.04062, ICML\u20192018, 2018.","link":"https:\/\/arxiv.org\/abs\/1801.04062","notes":"Very cool approach. Makes me want to dig more into information theory-based ML papers. Gotta see what this whole Information Bottleneck thing is.","last_updated":1575046283.3221960068,"created_at":1575045958.8890528679,"id":"44a2dfb3-2e1c-4f99-8e97-d66e35686c1a"}],"last_updated":1575046283.3221979141}